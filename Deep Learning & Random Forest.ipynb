{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing, metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "train_tour1 = pd.read_csv('numerai_training_data.csv')\n",
    "\n",
    "#separate target and features\n",
    "data_feature = np.asarray(train_tour1.ix[:,0:1])\n",
    "data_target = np.asarray( train_tour1.target)\n",
    "\n",
    "# convert list of labels to binary class matrix\n",
    "target = np_utils.to_categorical(data_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(data_feature)\n",
    "data_feature /= scale\n",
    "\n",
    "mean = np.std(data_feature)\n",
    "data_feature -= mean\n",
    "\n",
    "input_dim = data_feature.shape[1]\n",
    "nb_classes = target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dimensions: 1\n",
      "target dimensions: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"input dimensions:\", input_dim)\n",
    "print(\"target dimensions:\", nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 28885 samples, validate on 28886 samples\n",
      "Epoch 1/10\n",
      "28885/28885 [==============================] - 2s - loss: 0.6934 - val_loss: 0.6937\n",
      "Epoch 2/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6934 - val_loss: 0.6930\n",
      "Epoch 3/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6933 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6933 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6932 - val_loss: 0.6930\n",
      "Epoch 6/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "28885/28885 [==============================] - 2s - loss: 0.6932 - val_loss: 0.6933\n",
      "Epoch 9/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6932 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "28885/28885 [==============================] - 1s - loss: 0.6932 - val_loss: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2d2773710>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "# Deep Learning 2 Layers\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "## layer 2\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(data_feature, target, nb_epoch=10, batch_size=20, validation_split=0.5, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
